# -*- coding: utf-8 -*-
"""Text_Clustering_Group15_Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fWSChU8VrnBQ8dtunNsA86kzfR2O1uAq

**Group 15 Assignment 2 text clustering**

#Step1: Preparing our books and data
"""

# Commented out IPython magic to ensure Python compatibility.
#Install mlxtend which is a library of Python tools and extensions for data science.
# %pip install mlxtend --upgrade

#import nltk---->leading platform for building Python programs to work with human language data.
import nltk
nltk.download('punkt')    #downloading punctuations from NLTK
nltk.download("stopwords")  #download stopwords from NLTK
nltk.download("wordnet")  #downloading lemmatizers from NLTK
from nltk.corpus import gutenberg #a small selection of texts from the Project Gutenberg electronic text archive
from nltk.corpus import stopwords #importing stopwords
from nltk.stem import WordNetLemmatizer # algorithmic process of finding the lemma of a word depending on its meaning and context.

import math #This module provides access to the mathematical functions defined by the C standard.
import pandas as pd  #open source data analysis library 
import numpy as np   #a Python library used for working with arrays.
import re #a Python library used for working with regular expressions

import matplotlib.pyplot as plt #to make some plots to discover our data and results
import tensorflow as tf

from sklearn import preprocessing
from sklearn.utils import shuffle
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.model_selection import learning_curve
from sklearn.model_selection import ShuffleSplit
from mlxtend.evaluate import bias_variance_decomp

#Urllib package is the URL handling module for python. It is used to fetch URLs (Uniform Resource Locators). 
#It uses the urlopen function and is able to fetch URLs using a variety of different protocols.
from urllib import request

#combine all URLs of our historical books on a list 
BooksURLs = ["https://www.gutenberg.org/files/24654/24654-8.txt" ,
             "https://www.gutenberg.org/files/27785/27785-8.txt" ,
             "https://www.gutenberg.org/files/14558/14558.txt" ,
             "https://www.gutenberg.org/files/2609/2609-0.txt" ,
             "https://www.gutenberg.org/files/28247/28247-8.txt"]

#make a list of book names and make a label for every book
BooksNames = ["Chaldea",
              "A Book About Lawyers",
              "EBook of Darwinism",
              "The Vicomte de Bragelonne",
              "A Popular History of Astronomy During the Nineteenth Century"]
BooksLabels = ["a", "b" ,"c", "d" , "e"]

#make a list of book authors
BooksAuthors = ["Zénaïde A. Ragozin",
                "John Cordy",
                "Alfred Russel Wallace",
                "Alexandre Dumas, Père",
                "Agnes M. (Agnes Mary) Clerke"]

Books=[]

"""#Step2: preprocessing the data and data cleaning"""

import nltk
 nltk.download('omw-1.4')

from urllib import request
#for loop to get every book in BooksURLs list
for URL  in BooksURLs :
  response = request.urlopen(URL)
  raw = response.read().decode('utf8' , errors = 'replace')
  wordsList= re.findall(r"[a-zA-Z]{3,}", raw)
  #perform lemmetization on the data
  lemmatizer = WordNetLemmatizer()
  lemmitizedWords =[]
  for i in wordsList:
    words = i.lower()
    word = lemmatizer.lemmatize(words)
    #check if the word not in stopwords set
    if word not in set(stopwords.words('english')):
      lemmitizedWords.append(str(word))
  Books.append(lemmitizedWords)

#to ensure that every book have 200 partition, and every partition have 150 words.
BooksWords = []  #to combine all words together
#for loop to get the book             
for i in Books:
  l = i[0:(math.floor(len(i)/150)) * 150]
  BooksWords.append(l)
#to combine all lists of the words on a single dataframe
result = pd.DataFrame()
for i in range(len(BooksWords)):
    df = {}
    list_of_partitions =  [BooksWords[i][x:x+150] for x in range(0, len(BooksWords[i]), 150)]
    df['index'] = i
    #to combine Book Authors in one column
    df['Author_of_Book']= BooksAuthors[i]
    #to combine Book Names in one column
    df['Title_of_Book']= BooksNames[i] 
    #to combine Book Labels in one column
    df['Label_of_Book'] = BooksLabels[i]
    #to combine Book Partitions in one column
    df['PartitionsList'] = list_of_partitions 
    data = pd.DataFrame(df)
    #for loop to join our data together
    for i in range(len(data)):
      data["PartitionsList"][i] = " ".join(data["PartitionsList"][i])
    final_result = data[:200]
    result = result.append(final_result)
#shuffle the result of combining all dataframes together
result = shuffle(result)

#print head of data
result.head(5)

#describe our result
result.describe()

#get some information about our result
result.info()

import seaborn as sns
plt.figure(figsize=(10,10))
ax = sns.countplot(x=result["Label_of_Book"],  data=result, order = result["Label_of_Book"].value_counts().index )
for p, label in zip(ax.patches, result["Label_of_Book"].value_counts()):   
    ax.annotate(label, (p.get_x()+0.25, p.get_height()+0.5))

top= 10
for label in result['Title_of_Book'].unique():
    corpus = result[result["Title_of_Book"]==label]["PartitionsList"]
    lst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=" "))
    fig, ax = plt.subplots(nrows=1, ncols=2,constrained_layout=True, figsize=(20, 4))
    fig.suptitle(f"THE MOST FREQUENT WORDS OF BOOK: {label} ", fontsize=15)

    #to draw the unigram gragh
    dic_words_freq = nltk.FreqDist(lst_tokens)
    result_unigram = pd.DataFrame(dic_words_freq.most_common(), 
                        columns=["Word","Freq"])
    result_unigram.set_index("Word").iloc[:10,:].sort_values(by="Freq").plot(
                    kind="barh", title="Unigram", ax=ax[0], 
                    legend=False).grid(axis='x')
    ax[0].set(ylabel=None)
    
    #to draw the bigram graph
    dic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))
    result_bigram = pd.DataFrame(dic_words_freq.most_common(), 
                        columns=["Word","Freq"])
    result_bigram["Word"] = result_bigram["Word"].apply(lambda x: " ".join(
                    string for string in x) )
    result_bigram.set_index("Word").iloc[:10,:].sort_values(by="Freq").plot(
                    kind="barh", title="Bigrams", ax=ax[1],
                    legend=False).grid(axis='x')
    ax[1].set(ylabel=None)

"""make a wordcloud figure to get the most frequent 50 words in every book"""

import wordcloud #Python wordcloud library to create tag clouds
import string
#for loop to take every unique book in the column of Title_of_Book 
for n in result['Title_of_Book'].unique():
  books = result[result["Title_of_Book"]==n]["PartitionsList"]
 #to print the most frequent 50 words of the unique book
  print(f"\n THE MOST FREQUENT 50 WORDS OF BOOK CALLED: {n}\n")
  WordCloudGragh = wordcloud.WordCloud(background_color='black', max_words=50, max_font_size=40)
  WordCloudGragh = WordCloudGragh.generate(str(books))
  plt.axis('off')
  plt.imshow(WordCloudGragh, cmap=None)
  plt.show()

"""#Step3: perform data transformation


1.   BOW
2.   TF-IDF
3.   LDA
4.   Word2Vec

## **1- BOW Transformation**
"""

from sklearn.feature_extraction.text import CountVectorizer #to Convert a collection of text documents to a matrix of token counts.

countVector= CountVectorizer()

"""A bag of words is a representation of text that describes the occurrence of words within a document."""

#perform BOW transformation on the partitionslist column
BOWVector = countVector.fit_transform(result['PartitionsList'])

BOW = pd.DataFrame(BOWVector.toarray(), columns=countVector.get_feature_names())

#print Bag Of Words
BOW

"""## **2- TF-IDF Transformation**"""

#to Convert a collection of raw documents to a matrix of TF-IDF features.
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()

"""Term frequency (TF) vectors show how important words are to documents. They are computed by using: ![image.png](data:image/png;base64,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)"""

#to Transform result to document-term matrix.
TFIDF = vectorizer.fit_transform(result['PartitionsList'])

#transform it to array
TFIDF_Vector = pd.DataFrame(TFIDF.toarray(), columns=vectorizer.get_feature_names())

#print the TF-IDF vector
TFIDF_Vector

"""## **3- LDA Transformation**

LDA is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions. Each document is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words
"""

import gensim
from gensim import models
from gensim.models.wrappers import LdaMallet
from gensim import corpora, models, similarities
from sklearn.decomposition import LatentDirichletAllocation

# create a dictionary of the book words
wordsDictionary = corpora.Dictionary(result['PartitionsList'].str.split())
corpus = [wordsDictionary.doc2bow(wordDic) for wordDic in result['PartitionsList'].str.split()]

#creating the LDA model
LDA_Model = gensim.models.ldamodel.LdaModel(corpus = corpus,
                                           id2word = wordsDictionary,
                                           num_topics = 150,
                                           random_state = 100,
                                           update_every = 1,
                                           chunksize = 100, 
                                           passes = 10, 
                                           alpha = 'auto',
                                           per_word_topics = True)
predictionWords = LDA_Model.inference(corpus)

!pip install pyLDAvis

# create a dictionary of the book words
wordsDictionary = corpora.Dictionary(result['PartitionsList'].str.split())
corpus = [wordsDictionary.doc2bow(wordDic) for wordDic in result['PartitionsList'].str.split()]

LDAModel = gensim.models.ldamodel.LdaModel(corpus = corpus,
                                           id2word = wordsDictionary,
                                           num_topics = 5,
                                           random_state = 100,
                                           update_every = 1,
                                           chunksize = 100, 
                                           passes = 10, 
                                           alpha = 'auto',
                                           per_word_topics = True)
PredictedWords = LDAModel.inference(corpus)

import pyLDAvis
import pyLDAvis.gensim_models as genisvis
pyLDAvis.enable_notebook()
visualization = genisvis.prepare(LDAModel, corpus, wordsDictionary)
visualization

LDA = pd.DataFrame(list(PredictedWords[0]), columns=['1', '2','3','4', '5'])

LDA['res'] = LDA.idxmax(axis = 1)

LDA.head()

PredictedWords

"""## **4- Word Embedding**

Word2Vec consists of models for generating word embedding. These models are shallow two-layer neural networks having one input layer, one hidden layer, and one output layer.
"""

from gensim.models import Word2Vec

# split our partitionsList to a list of words
PartitionsToSplit = result['PartitionsList'].tolist()
SplittedList = [x.split() for x in PartitionsToSplit]

print(len(SplittedList))

print(len(SplittedList[0]))

print(SplittedList[0])

# build our word2vec model
word_2_vec_model = Word2Vec(SplittedList,
                            min_count= 1,
                            size= 150,
                            workers=6, 
                            window= 3,
                            sg= 1)
# save trained model
word_2_vec_model.save("word2vec.model")

def vectorize(list_of_docs, model):
    features = []

    for tokens in list_of_docs:
        zero_vector = np.zeros(model.vector_size)
        vectors = []
        for token in tokens:
            if token in model.wv:
                try:
                    vectors.append(model.wv[token])
                except KeyError:
                    continue
        if vectors:
            vectors = np.asarray(vectors)
            avg_vec = vectors.mean(axis=0)
            features.append(avg_vec)
        else:
            features.append(zero_vector)
    return features
    
vectorized_docs = vectorize(SplittedList, model=word_2_vec_model)
X_emb = np.array(vectorized_docs)
len(vectorized_docs), len(vectorized_docs[0])

print(vectorized_docs[0])

"""# Step 4: Building the clustering algorithms


1.   K-means clustering algorithm
2.   Expectation Maximization(EM) algorithm
3.   Hierarchical clustering algorithm

T-distributed Stochastic Neighbor Embedding: for dimensionality reduction
Dimensionality reduction: is a good way to deal with the data that have many features, the TSNE is a good choice here.
TSNE used to reduce the number of dimensions to a reasonable amount if the number of features is very high.
"""

from sklearn.manifold import TSNE
def TSNEData(DesiredOutput):
  Tsna = TSNE(n_components= 2, random_state= 42)
  DataOfTSNE = Tsna.fit_transform(DesiredOutput) 
  return DataOfTSNE

"""## **1- Building K-means algorithm**"""

#Yellowbrick is a suite of visual analysis and diagnostic tools designed to facilitate machine learning with scikit-learn
!pip install yellowbrick
from yellowbrick.cluster import KElbowVisualizer

from sklearn.cluster import KMeans
def BuildingKMeansModel(clusters, X_data):
  kMeansModel= KMeans(n_clusters= clusters, init='k-means++', random_state=0)
  Y_Prediction = kMeansModel.fit_predict(X_data)
  return kMeansModel, Y_Prediction

"""## **2- Building Expectation Maximization(EM) algorithm**

Expectation-Maximization algorithm can be used for the latent variables (variables that are not directly observable and are actually inferred from the values of the other observed variables) too in order to predict their values with the condition that the general form of probability distribution governing those latent variables is known to us. This algorithm is actually at the base of many unsupervised clustering algorithms in the field of machine learning.
"""

from sklearn.mixture import GaussianMixture
def BuildingExpectationMaximization(clusters, X_data):
  gm= GaussianMixture(n_components=clusters ,covariance_type= "spherical" ,n_init= 10 )
  Y_Prediction= gm.fit_predict(X_data)
  ProbabilityOfGM = gm.predict_proba(X_data)
  return gm, Y_Prediction, ProbabilityOfGM

"""## **3- Building Hierarchical clustering algorithm**

Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.
"""

from scipy.cluster.hierarchy import dendrogram, linkage
from matplotlib import pyplot as plt
import scipy.cluster.hierarchy as shc
from sklearn.cluster import AgglomerativeClustering
def BuildingHierarchicalClustering(clusters, X_data, title):
  plt.figure(figsize=(10, 7)) 
  plt.title(title) 
  dend= shc.dendrogram(shc.linkage(X_data, method='ward'))
  ag= AgglomerativeClustering(n_clusters=clusters, affinity='euclidean', linkage='ward')  
  Y_Prediction= ag.fit_predict(X_data)
  return ag, Y_Prediction

"""## **Make a function for clustering Visualization**"""

from scipy.sparse import csr_matrix
from numpy import concatenate

def ClusteringVisualization(ClusteringModel, result, Y_Prediction, em=True):
  # get centroids of kmeans cluster
  if em:
    centroids = np.empty(shape=(ClusteringModel.n_components, result.shape[1]))
  else:
    centroids = ClusteringModel.cluster_centers_

  # we want to transform the rows and the centroids
  # todense return matrix
  matrix_data = csr_matrix(result)
  all_Data = concatenate((matrix_data.todense(), centroids))

  n_clusters = 5

  plt.scatter([all_Data[:-n_clusters, 0]], [all_Data[:-n_clusters, 1]], c=Y_Prediction, cmap=plt.cm.Paired, marker= 'x')
  plt.scatter([all_Data[-n_clusters:, 0]], [all_Data[-n_clusters:, 1]], marker= 'o')
  plt.show()

from collections import Counter
def CountingClusters(predictedClusters):
  clustersNumber = Counter(predictedClusters)
  plt.bar(clustersNumber.keys(), clustersNumber.values())

"""##Building the three algorithms with the four Transformations

## **1- Building BOW with the 3 algorithms**

### **1.1 Building BOW with K-Means algorithm**
"""

DataOfBOW = TSNEData(BOW)
KMeansWithBOW, KMeansWithBOWPrediction = BuildingKMeansModel(5,DataOfBOW)

KMeansWithBOW.cluster_centers_.argsort()[:, ::-1]

ClusteringVisualization(KMeansWithBOW, DataOfBOW, KMeansWithBOWPrediction,False)

CountingClusters(KMeansWithBOWPrediction)

"""### **1.2 Building BOW with Expectation Maximization(EM) algorithm**"""

ExpectationMaximizationWithBOW, ExpectationMaximizationWithBOWPrediction, ExpectationMaximizationWithBOWProbability = BuildingExpectationMaximization(5,DataOfBOW)

ClusteringVisualization(ExpectationMaximizationWithBOW, DataOfBOW, ExpectationMaximizationWithBOWPrediction,True)

CountingClusters(ExpectationMaximizationWithBOWPrediction)

"""### **1.3 Building BOW with Hierarchical clustering algorithm**"""

AgglomerativeClusteringWithBOW, AgglomerativeClusteringWithBOWPrediction= BuildingHierarchicalClustering(5, DataOfBOW,"Dendogram with BOW Transformation")

CountingClusters(AgglomerativeClusteringWithBOWPrediction)

"""## **2- Building TF-IDF with the 3 algorithms**

### **2.1 Building TF-IDF with K-Means algorithm**
"""

DataOfTFIDF = TSNEData(TFIDF_Vector)
# tfidf_data_pca = data_with_pca(X_tfidf)
KMeansWithTFIDF, KMeansWithTFIDFPrediction = BuildingKMeansModel(5,DataOfTFIDF)

ClusteringVisualization(KMeansWithTFIDF, DataOfTFIDF, KMeansWithTFIDFPrediction,False)

CountingClusters(KMeansWithTFIDFPrediction)

"""### **2.2 Building TF-IDF with Expectation Maximization(EM) algorithm**"""

ExpectationMaximizationWithTFIDF, ExpectationMaximizationWithTFIDFPrediction, ExpectationMaximizationWithTFIDFProbability = BuildingExpectationMaximization(5,DataOfTFIDF)

ClusteringVisualization(ExpectationMaximizationWithTFIDF, DataOfTFIDF, ExpectationMaximizationWithTFIDFPrediction,True)

CountingClusters(ExpectationMaximizationWithTFIDFPrediction)

"""### **2.3 Building TF-IDF with Hierarchical clustering algorithm**"""

HierarchicalClusteringWithTFIDF, HierarchicalClusteringWithTFIDFPrediction= BuildingHierarchicalClustering(5, DataOfTFIDF,"Dendogram with TFIDF Transformation")

CountingClusters(HierarchicalClusteringWithTFIDFPrediction)

"""## **3- Building LDA with the 3 algorithms**

### **3.1 Building LDA with K-Means algorithm**
"""

DataOfLDA = TSNEData(PredictedWords[0])
KMeansWithLDA, KMeansWithLDAPrediction = BuildingKMeansModel(5, DataOfLDA)

ClusteringVisualization(KMeansWithLDA, DataOfLDA, KMeansWithLDAPrediction,False)

CountingClusters(KMeansWithLDAPrediction)

"""### **3.2 Building LDA with Expectation Maximization(EM) algorithm**"""

LDAWithExpectationMaximization, LDAWithExpectationMaximizationPrediction, LDAWithExpectationMaximizationProbability = BuildingExpectationMaximization(5, DataOfLDA)

ClusteringVisualization(LDAWithExpectationMaximization, DataOfLDA, LDAWithExpectationMaximizationPrediction,True)

CountingClusters(LDAWithExpectationMaximizationPrediction)

"""### **3.3 Building LDA with Hierarchical clustering algorithm**"""

HierarchicalClusteringWithLDA, HierarchicalClusteringWithLDAPrediction= BuildingHierarchicalClustering(5, DataOfLDA, "Dendogram with LDA Transformation")

CountingClusters(HierarchicalClusteringWithLDAPrediction)

"""## **4- Building Word2Vec with the 3 algorithms**

### **4.1 Building Word2Vec with K-Means algorithm**
"""

Wored2VecData = TSNEData(X_emb)
Wored2VecDataWithKMeans, Wored2VecDataWithKMeansPrediction = BuildingKMeansModel(5,Wored2VecData)

ClusteringVisualization(Wored2VecDataWithKMeans, Wored2VecData, Wored2VecDataWithKMeansPrediction,False)

CountingClusters(Wored2VecDataWithKMeansPrediction)

"""### **4.2 Building Word2Vec with Expectation Maximization(EM) algorithm**"""

Word2VecWithExpectationMaximizationing, Word2VecWithExpectationMaximizationingPrediction, Word2VecWithExpectationMaximizationingProbability = BuildingExpectationMaximization(5,Wored2VecData)

ClusteringVisualization(Word2VecWithExpectationMaximizationing, Wored2VecData, Word2VecWithExpectationMaximizationingPrediction,True)

CountingClusters(Word2VecWithExpectationMaximizationingPrediction)

"""### **4.3 Building Word2Vec with Hierarichal Clustering algorithm**"""

Word2VecWithHierarchicalClustering, Word2VecWithHierarchicalClusteringPrediction= BuildingHierarchicalClustering(5, Wored2VecData,"Dendogram with Word2Vec Transformation")

CountingClusters(Word2VecWithHierarchicalClusteringPrediction)

"""# **Step 5: Perform Evaluation**

## **1- Perform Kappa Evaluation against the true authors**
"""

def majorityClass(rater_1, rater_2, k):
    label_cluster = 0
    Max = []
    for i in range(k):
        counter0= 0
        counter1=0
        counter2=0
        counter3=0
        counter4=0
        L = [] 
        for j in range(k):  
            for m in range(len(rater_2)):
                if (rater_1[m] == label_cluster) & (rater_2[m] == j) & (j == 0):
                    counter0 = counter0 + 1
                if (rater_1[m] == label_cluster) & (rater_2[m] == j) & (j == 1):
                    counter1 = counter1 + 1
                if (rater_1[m] == label_cluster) & (rater_2[m] == j) & (j == 2):
                    counter2 = counter2 + 1
                if (rater_1[m] == label_cluster) & (rater_2[m] == j) & (j == 3):
                    counter3 = counter3 + 1
                if (rater_1[m] == label_cluster) & (rater_2[m] == j) & (j == 4):
                    counter4 = counter4 + 1
        print("True_0 in cluster "+ str(label_cluster)+": " + str(counter0))
        print("True_0 in cluster "+ str(label_cluster)+": " +str(counter1))
        print("True_0 in cluster "+ str(label_cluster)+": " +str(counter2))
        print("True_0 in cluster "+ str(label_cluster)+": " +str(counter3))
        print("True_0 in cluster "+ str(label_cluster)+": " +str(counter4))
        L.append(counter0)
        L.append(counter1)
        L.append(counter2)
        L.append(counter3)
        L.append(counter4)
        idx = np.argmax(L)
        print(idx)
        Max.append(idx)
        label_cluster = label_cluster + 1
    return Max

def mapLabels(rater_1, Max, k):
    indices = [True]*len(rater_1)
    for l in range(k):
        for n in range(len(rater_1)):
            if indices[n] == True: 
                if rater_1[n] == l:
                    rater_1[n] = Max[l]
                    indices[n] = False
    return rater_1

from sklearn.metrics import cohen_kappa_score

LDAPrediction = LDA.loc[:,'res'].values

LDAPrediction = LDAPrediction.astype(int)

LDAPrediction

"""### **1- Perform Kappa with K-means clustering Model**"""

Max = majorityClass(list(KMeansWithBOWPrediction), list(result['index']), 5)
KMeansWithBOWPrediction = mapLabels(list(KMeansWithBOWPrediction), Max, 5)

Max = majorityClass(list(KMeansWithTFIDFPrediction), list(result['index']), 5)
KMeansWithTFIDFPrediction = mapLabels(list(KMeansWithTFIDFPrediction), Max, 5)

Max = majorityClass(list(KMeansWithLDAPrediction), list(result['index']), 5)
KMeansWithLDAPrediction = mapLabels(list(KMeansWithLDAPrediction), Max, 5)

Max = majorityClass(list(Wored2VecDataWithKMeansPrediction), list(result['index']), 5)
Wored2VecDataWithKMeansPrediction = mapLabels(list(Wored2VecDataWithKMeansPrediction), Max, 5)

print("Kappa Score of K-means With BOW Transformation:  {:.5f}".format(cohen_kappa_score(result['index'],KMeansWithBOWPrediction)))
print("Kappa Score of K-means With TFIDF Transformation:  {:.5f}".format(cohen_kappa_score(result['index'],KMeansWithTFIDFPrediction)))
print("Kappa Score of K-means With LDA Transformation:  {:.5f}".format(cohen_kappa_score(result['index'],KMeansWithLDAPrediction)))
print("Kappa Score of K-means With Word2Vec Transformation:  {:.5f}".format(cohen_kappa_score(result['index'],Wored2VecDataWithKMeansPrediction)))

print("Kappa Score of LDA as Topic Modeling:  {:.5f}".format(cohen_kappa_score(result['index'], LDAPrediction)))

"""### **2- Perform Kappa with Expectation Maximization(EM) clustering Model**"""

Max = majorityClass(list(ExpectationMaximizationWithBOWPrediction), list(result['index']), 5)
ExpectationMaximizationWithBOWPrediction = mapLabels(list(ExpectationMaximizationWithBOWPrediction), Max, 5)

Max = majorityClass(list(ExpectationMaximizationWithTFIDFPrediction), list(result['index']), 5)
ExpectationMaximizationWithTFIDFPrediction = mapLabels(list(ExpectationMaximizationWithTFIDFPrediction), Max, 5)

Max = majorityClass(list(LDAWithExpectationMaximizationPrediction), list(result['index']), 5)
LDAWithExpectationMaximizationPrediction = mapLabels(list(LDAWithExpectationMaximizationPrediction), Max, 5)

Max = majorityClass(list(Word2VecWithExpectationMaximizationingPrediction), list(result['index']), 5)
Word2VecWithExpectationMaximizationingPrediction = mapLabels(list(Word2VecWithExpectationMaximizationingPrediction), Max, 5)

print("Kappa Score of Expectation Maximization With BOW Transformation:  {:.5f}".format(cohen_kappa_score(result['index'],ExpectationMaximizationWithBOWPrediction)))
print("Kappa Score of Expectation Maximization With TFIDF Transformation:  {:.5f}".format(cohen_kappa_score(result['index'],ExpectationMaximizationWithTFIDFPrediction)))
print("Kappa Score of Expectation Maximization With LDA Transformation:  {:.5f}".format(cohen_kappa_score(result['index'],LDAWithExpectationMaximizationPrediction)))
print("Kappa Score of Expectation Maximization With Word2Vec Transformation:  {:.5f}".format(cohen_kappa_score(result['index'],Word2VecWithExpectationMaximizationingPrediction)))

"""### **3- perform Kappa with Hierarchical clustering Model**

Cohen’s kappa: a statistic that measures inter-annotator agreement.
"""

Max = majorityClass(list(AgglomerativeClusteringWithBOWPrediction), list(result['index']), 5)
AgglomerativeClusteringWithBOWPrediction = mapLabels(list(AgglomerativeClusteringWithBOWPrediction), Max, 5)

Max = majorityClass(list(HierarchicalClusteringWithTFIDFPrediction), list(result['index']), 5)
HierarchicalClusteringWithTFIDFPrediction = mapLabels(list(HierarchicalClusteringWithTFIDFPrediction), Max, 5)

Max = majorityClass(list(Word2VecWithHierarchicalClusteringPrediction), list(result['index']), 5)
Word2VecWithHierarchicalClusteringPrediction = mapLabels(list(Word2VecWithHierarchicalClusteringPrediction), Max, 5)

Max = majorityClass(list(HierarchicalClusteringWithLDAPrediction), list(result['index']), 5)
HierarchicalClusteringWithLDAPrediction = mapLabels(list(HierarchicalClusteringWithLDAPrediction), Max, 5)

print("Kappa Score of Hierarchical clustering With BOW  Transformation:  {:.5f}".format(cohen_kappa_score(result['index'],AgglomerativeClusteringWithBOWPrediction)))
print("Kappa Score of Hierarchical clustering With TFIDF Transformation:  {:.5f}".format(cohen_kappa_score(result['index'],HierarchicalClusteringWithTFIDFPrediction)))
print("Kappa Score of Hierarchical clustering With Word2Vec Transformation:  {:.5f}".format(cohen_kappa_score(result['index'],Word2VecWithHierarchicalClusteringPrediction)))
print("Kappa Score of Hierarchical clustering With LDA Transformation:  {:.5f}".format(cohen_kappa_score(result['index'],HierarchicalClusteringWithLDAPrediction)))

"""## **2- Perform consistency with the V-Score Evaluation against the true authors**

### **1- Perform consistency with K-means clustering Model**

V-measure cluster labeling given a ground truth.
"""

from sklearn.metrics.cluster import v_measure_score
v_measure_score(result['index'],KMeansWithBOWPrediction)

v_measure_score(result['index'],KMeansWithTFIDFPrediction)

v_measure_score(result['index'],KMeansWithLDAPrediction)

v_measure_score(result['index'],Wored2VecDataWithKMeansPrediction)

"""### **2- Perform consistency with Expectation Maximization(EM) clustering Model**"""

v_measure_score(result['index'],ExpectationMaximizationWithBOWPrediction)

v_measure_score(result['index'],ExpectationMaximizationWithTFIDFPrediction)

v_measure_score(result['index'],LDAWithExpectationMaximizationPrediction)

v_measure_score(result['index'],Word2VecWithExpectationMaximizationingPrediction)

"""### **3- perform consistency with Hierarchical clustering Model**"""

v_measure_score(result['index'],AgglomerativeClusteringWithBOWPrediction)

v_measure_score(result['index'],HierarchicalClusteringWithTFIDFPrediction)

v_measure_score(result['index'],Word2VecWithHierarchicalClusteringPrediction)

v_measure_score(result['index'],HierarchicalClusteringWithLDAPrediction)

"""## **3- Perform Coherence Evaluation against the true authors**"""

from gensim.models.coherencemodel import CoherenceModel
from gensim.models import CoherenceModel

print('\nPerplexity score: ', LDA_Model.log_perplexity(corpus))

CoheranceModelEvaluation = CoherenceModel(model= LDA_Model, texts= SplittedList, dictionary=wordsDictionary, coherence='c_v')
CoheranceModelEvaluationUmass = CoherenceModel(model= LDA_Model, texts= SplittedList, dictionary=wordsDictionary, coherence='u_mass')

CoheranceModelValue = round(CoheranceModelEvaluation.get_coherence(), 4)  # get coherence value
CoheranceModelUmass = round(CoheranceModelEvaluationUmass.get_coherence(), 4)  # get coherence value

print("\nCoherence score With LDA using c_v: {}".format(CoheranceModelValue))
print("\nCoherence score With LDA using u_mass: {}".format(CoheranceModelUmass))

print('\nPerplexity score: ', LDAModel.log_perplexity(corpus))

CoheranceModelEvaluation = CoherenceModel(model= LDAModel, texts= SplittedList, dictionary=wordsDictionary, coherence='c_v')
CoheranceModelEvaluationUmass = CoherenceModel(model= LDAModel, texts= SplittedList, dictionary=wordsDictionary, coherence='u_mass')

CoheranceModelValue = round(CoheranceModelEvaluation.get_coherence(), 4)  # get coherence value
CoheranceModelUmass = round(CoheranceModelEvaluationUmass.get_coherence(), 4)  # get coherence value

print("\nCoherence score With LDA using c_v: {}".format(CoheranceModelValue))
print("\nCoherence score With LDA using u_mass: {}".format(CoheranceModelUmass))

"""## **4- Perform Silhouette Evaluation against the true authors**

Silhouette score is used to evaluate the quality of clusters created using clustering algorithms such as K-Means in terms of how well samples are clustered with other samples that are similar to each other. The Silhouette score is calculated for each sample of different clusters. 
Silhouette Coefficient or silhouette score is a metric used to calculate the goodness of a clustering technique. Its value ranges from -1 to 1.
"""

from sklearn.metrics import silhouette_score
from yellowbrick.cluster import SilhouetteVisualizer

def KMeansSilhouetteVisualization(X_data, model, title):
  visualizerOfSilhouette = SilhouetteVisualizer(model, colors='yellowbrick')
  visualizerOfSilhouette.fit(X_data)
  plt.title("Silhouette Score of K-means With " + title)

"""### **3.1 Perform Silhouette evaluation with K-Means clustering model**"""

print("Silhouette Score of K-means With BOW Transformation:  {:.5f}".format(silhouette_score(DataOfBOW, KMeansWithBOWPrediction)))
print("Silhouette Score of K-means With TFIDF Transformation:  {:.5f}".format(silhouette_score(DataOfTFIDF, KMeansWithTFIDFPrediction)))
print("Silhouette Score of K-means With LDA Transformation:  {:.5f}".format(silhouette_score(DataOfLDA, KMeansWithLDAPrediction)))
print("Silhouette Score of K-means With Word2Vec Transformation:  {:.5f}".format(silhouette_score(Wored2VecData, Wored2VecDataWithKMeansPrediction)))

"""#### **3.1.1 Perform Silhouette evaluation with K-Means clustering model with BOW Transformation**"""

KMeansSilhouetteVisualization(DataOfBOW, KMeansWithBOW,"BOW Transformation");

"""#### **3.1.2 Perform Silhouette evaluation with K-Means clustering model with TFIDF Transformation**"""

KMeansSilhouetteVisualization(DataOfTFIDF, KMeansWithTFIDF, "TFIDF Transformation");

"""#### **3.1.3 Perform Silhouette evaluation with K-Means clustering model with LDA Transformation**"""

KMeansSilhouetteVisualization(DataOfLDA, KMeansWithLDA,"LDA Transformation");

"""#### **3.1.4 Perform Silhouette evaluation with K-Means clustering model with Word2Vec Transformation**"""

KMeansSilhouetteVisualization(Wored2VecData, Wored2VecDataWithKMeans, "Word2Vec Transformation");

"""### **3.2 Perform Silhouette evaluation with Expectation Maximization (EM) clustering model**"""

print("Silhouette Score of Expectation Maximization With BOW Transformation:  {:.5f}".format(silhouette_score(DataOfBOW, ExpectationMaximizationWithBOWPrediction)))
print("Silhouette Score of Expectation Maximization With TFIDF Transformation:  {:.5f}".format(silhouette_score(DataOfTFIDF, ExpectationMaximizationWithTFIDFPrediction)))
print("Silhouette Score of Expectation Maximization With LDA Transformation:  {:.5f}".format(silhouette_score(DataOfLDA, LDAWithExpectationMaximizationPrediction)))
print("Silhouette Score of Expectation Maximization With Word2Vec Transformation:  {:.5f}".format(silhouette_score(Wored2VecData, Word2VecWithExpectationMaximizationingPrediction)))

"""### **3.3 Perform Silhouette evaluation with Hierarchical clustering model**"""

print("Silhouette Score of Hierarchical clustering With BOW Transformation:  {:.5f}".format(silhouette_score(DataOfBOW, AgglomerativeClusteringWithBOWPrediction)))
print("Silhouette Score of Hierarchical clustering With TFIDF Transformation:  {:.5f}".format(silhouette_score(DataOfTFIDF, HierarchicalClusteringWithTFIDFPrediction)))
print("Silhouette Score of Hierarchical clustering With LDA Transformation:  {:.5f}".format(silhouette_score(DataOfLDA, HierarchicalClusteringWithLDAPrediction)))
print("Silhouette Score of Hierarchical clustering With Word2Vec Transformation:  {:.5f}".format(silhouette_score(Wored2VecData, Word2VecWithHierarchicalClusteringPrediction)))

"""# **Step 6: Error analysis**

## **6.1 Print the silhouette scores**
"""

from sklearn.metrics import silhouette_score ,silhouette_samples #to Compute the mean Silhouette Coefficient of all samples.
import matplotlib.cm as cm

RangeOfClusters = [ 3, 4, 5, 6, 7, 8] #define the range of clusters
SilhouetteScores = []

#for loop on the range of the clusters to draw the silhouette score and samples 
for NumOfClusters in RangeOfClusters:
    #To create a subplot of 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(13, 4)
    ax1.set_xlim([-0.1, 1])
    ax1.set_ylim([0, len(DataOfLDA) + (NumOfClusters + 1) * 10])
    #define the clusters of K-means clustering model
    clusters = KMeans(n_clusters=NumOfClusters, random_state=10)
    #to define the cluster labels to predict
    cluster_labels = clusters.fit_predict(DataOfLDA)
    AverageOfSilhouette = silhouette_score(DataOfLDA, cluster_labels)
    SilhouetteScores.append(AverageOfSilhouette)
    #to print thr average number of silhouette score
    print("For NumOfClusters =", NumOfClusters, "The average of the silhouette score is :", AverageOfSilhouette ,"\n")
    #To compute the silhouette scores for each sample
    SilhouetteValuesSamples = silhouette_samples(DataOfLDA, cluster_labels)
    y_lower = 10
    for i in range(NumOfClusters):
        SilhouetteValues = \
            SilhouetteValuesSamples[cluster_labels == i]
        #sort the values of sillhouette 
        SilhouetteValues.sort()
        ClusterSize = SilhouetteValues.shape[0]
        #set the value of y_upper
        y_upper = y_lower + ClusterSize
        color = cm.nipy_spectral(float(i) / NumOfClusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, SilhouetteValues, facecolor=color, edgecolor=color, alpha=0.7)
        #define the labels of silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * ClusterSize, str(i))
        # Compute the new y_lower for next plot
        y_lower = y_upper + 10 

    ax1.set_title("The silhouette plot for the various clusters")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")
    ax1.axvline(x=AverageOfSilhouette, color="red", linestyle="--") #vertical line for the average silhouette score of all the values
    ax1.set_yticks([])  
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
    #to show the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / NumOfClusters)
    ax2.scatter(DataOfLDA[:, 0], DataOfLDA[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')
    #to label the clusters based on the center value
    centers = clusters.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',c="white", alpha=1, s=200, edgecolor='k') 
    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')
    #to set the titles of x and y labels
    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")
    plt.suptitle(("Silhouette analysis for K-Means clustering  " "with NumOfClusters = %d" % NumOfClusters ,"with average silhouette score:", AverageOfSilhouette ), fontsize=14, fontweight='bold')
plt.show()

"""## **6.2 Create analysis data frame for the true label and predicted label**"""

def ResultAnalysis (FeatureEngineeringResults , result):
  AnalysisDataFrame = result[['PartitionsList', 'Label_of_Book' , "index" ]]
  AnalysisDataFrame['clustersOutput'] = FeatureEngineeringResults
  return AnalysisDataFrame

"""## **6.3 count the number of matches between most frequent words in clusters and true labels**

"""

from collections import Counter
def MostCommonWords(NumberOfWords ,DataFrameName , Label , ResultColumn):
  wordsList = []
  #for loop to extract the unique labels
  for l in DataFrameName[Label].unique():
    wl = DataFrameName[DataFrameName[Label]==l][ResultColumn]
    wordsList.append(wl)
  clustersLabel= []
  for c in range(len(wordsList)):
    label = []
    for doc in wordsList[c]:
      text = ""
      text += doc
      text += " "
      label.append(text)
    clustersLabel.append(label)

  ContentOfCluster = []
  for i in range(len(clustersLabel)):
    AllText = ""
    for j in range(len(clustersLabel[i])):
      AllText += clustersLabel[i][j]
      AllText += " "
    ContentOfCluster.append(AllText)

  MostCommonWords = []
  WeightForMostCommonWords = []
 #for loop to Pass the classSplit list to the instance of Counter class.
  for cc in range(len(ContentOfCluster)):
    WordList =[]
    WeightList = []
    string = ContentOfCluster[cc]
    ClassSplit = string.split()               
    Count = Counter(ClassSplit)            
    MostOccurrance = Count.most_common(NumberOfWords)
    #define the most occurence weights 
    for m in range(len(MostOccurrance)): 
      mostOcc =  MostOccurrance[m][0]
      weights = MostOccurrance[m][1]
      WordList.append(mostOcc)
      WeightList.append(weights)

    MostCommonWords.append(WordList)
    WeightForMostCommonWords.append(WeightList)
  return MostCommonWords , WeightForMostCommonWords

"""## **6.3 Create a disctionary of matches labels and matches weights**"""

def MatchingLabels(MostCommonLabels , MostCommonClusters):
  LabelsClustersMatches = {}
  for i in range(len(MostCommonLabels)):
    score = []
    for j in range(len(MostCommonClusters)):
      matches = len(set(MostCommonLabels[i]) & set(MostCommonClusters[j]))
      score.append(matches)
    LabelsClustersMatches[i] = score 
  return LabelsClustersMatches
# Create  disctionary of matches weights
def WeightsDictionary(MostCommonLabels , MostCommonClusters ,MostCommonClustersWeights ):
  LabelsClustersMatchesWeights = {}
  for i in range(len(MostCommonLabels)):
    score = []
    for j in range(len(MostCommonClustersWeights)):
      matches = len(set(MostCommonLabels[i]) & set(MostCommonClusters[j]))
      weight = sum(MostCommonClustersWeights[j][0:matches])
      score.append(weight)
    LabelsClustersMatchesWeights[i] = score 
  return LabelsClustersMatchesWeights

#show a list of the most frequent words and there weights
def WeightForMostCommonWords(labelList , WList):
  MostFrequentWordsWeight = []
  for i  in range(len(labelList)):
    lst = []
    for x, y in zip(labelList[i], WList[i]):
      element = [x ,y]
      z= tuple(element)
      lst.append(z)
    MostFrequentWordsWeight.append(lst)   
  return MostFrequentWordsWeight

AnalysisDataFrame = ResultAnalysis(KMeansWithLDAPrediction , result)
AnalysisDataFrame

output = list(AnalysisDataFrame.clustersOutput) 
true_label = list(AnalysisDataFrame['index'])

print(output)

print(true_label)

L=[]
for i in range(len(true_label)):
  if output[i] != true_label[i]:
    L.append(i)
print(L)
AnalysisDataFrame = AnalysisDataFrame.iloc[L, :]
AnalysisDataFrame

MostCommonLabels , MostFrequentLabelsWeight  = MostCommonWords (25, AnalysisDataFrame, "index","PartitionsList")
MostCommonClusters,MostCommonClustersWeights = MostCommonWords( 25, AnalysisDataFrame, "clustersOutput", "PartitionsList")

LabelsClustersMatches = MatchingLabels(MostCommonLabels , MostCommonClusters)
LabelsClustersMatches

LabelsClustersWeights = WeightsDictionary(MostCommonLabels ,MostCommonClusters , MostCommonClustersWeights)
LabelsClustersWeights

CommonLabels = WeightForMostCommonWords(MostCommonLabels,MostFrequentLabelsWeight)
print("\n The Most Frequent words with its occurancies through the actual class \n")
print("most frequent words in label : 0  \n ", CommonLabels[0] ,"\n")
print("most frequent words in label : 1  \n ", CommonLabels[1] ,"\n")
print("most frequent words in label : 2  \n ", CommonLabels[2] ,"\n")
print("most frequent words in label : 3  \n ", CommonLabels[3] ,"\n")
print("most frequent words in label : 4  \n ", CommonLabels[4] ,"\n")

CommonLabelsWeights = WeightForMostCommonWords(MostCommonClusters,MostCommonClustersWeights)
print("\n The Most Frequent words with its occurancies through the actual cluster \n")
print("most frequent words in cluster : 0  \n ", CommonLabelsWeights[0] ,"\n")
print("most frequent words in cluster : 1  \n ", CommonLabelsWeights[1] ,"\n")
print("most frequent words in cluster : 2  \n ", CommonLabelsWeights[2] ,"\n")
print("most frequent words in cluster : 3  \n ", CommonLabelsWeights[3] ,"\n")
print("most frequent words in cluster : 4  \n ", CommonLabelsWeights[4] ,"\n")

AnalysisDataFrame

L=[]
for i in range(len(AnalysisDataFrame)):
  for j in range(i+1,len(AnalysisDataFrame)):
    record1 = set(AnalysisDataFrame.iloc[i, 0].split(' '))
    record2 = set(AnalysisDataFrame.iloc[j, 0].split(' '))
    intersect = record1.intersection(record2)
    L.append(intersect)
    print(intersect)

print(L)

"""## **6.4 Most common words with their repeat count in all records that were labelled uncorrectly according to the human label**"""

from collections import Counter
C = Counter(x for xs in L for x in xs)
print(C)

print("Most common words with their repeat count in all records that were labelled uncorrectly according to the human label:\n")
print(C.most_common(25))

import nltk
from nltk.collocations import *

nltk.download('genesis')
bigram_measures = nltk.collocations.BigramAssocMeasures()
L=[]
for i in range(len(AnalysisDataFrame)):
  for j in range(i+1,len(AnalysisDataFrame)):
    finder = BigramCollocationFinder.from_words(AnalysisDataFrame.iloc[i ,0].split(' '))
    record1 = set(finder.nbest(bigram_measures.pmi, 25))
    finder2 = BigramCollocationFinder.from_words(AnalysisDataFrame.iloc[j ,0].split(' '))
    record2 = set(finder2.nbest(bigram_measures.pmi, 25))
    intersect = record1.intersection(record2)
    if intersect:
      L.append(intersect)
      #print(intersect)
print(L)

C = Counter(x for xs in L for x in xs)
print(C)

print("Most common collocations with their repeat count in all records that were labelled uncorrectly according to the human label:\n")
print(C.most_common(20))

import matplotlib.pyplot as plt

y_axis = []
x_axis = []


for i in range(len(C.most_common(20))):
  y_axis.append(str(C.most_common(20)[i][0]))
  x_axis.append(C.most_common(20)[i][1])

plt.barh(y_axis,x_axis)
plt.title('Common Bigrams in instances that threw the machine off')
plt.xlabel('Repeat count of bigrams')
plt.ylabel('Bigrams')
plt.show()